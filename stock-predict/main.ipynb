{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Predict Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For Graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# For plotting learning curve\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Yahoo API\n",
    "import yfinance as yf\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "scaler = StandardScaler()\n",
    "config = {\n",
    "    'seed': 666999,\n",
    "    'select_all': False,\n",
    "    'valid_ratio': 0.2,\n",
    "    'test_ratio': 0.2,\n",
    "    'n_epochs': 5000,\n",
    "    'learning_rate': 1e-5,\n",
    "    'early_stop': 300,\n",
    "    'save_path': './models/stock.ckpt',\n",
    "    'data_loader': {\n",
    "        'batch_size': 128,\n",
    "        'pin_memory': True,\n",
    "        'num_workers': 0,\n",
    "        'shuffle': True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "def predict(test_loader, model, device):\n",
    "    model.eval()\n",
    "    result = {'features': [], 'preds': [], 'ans': []}\n",
    "    for x, y in tqdm(test_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            result['features'].append(x.detach().cpu())\n",
    "            result['preds'].append(pred.detach().cpu())\n",
    "            result['ans'].append(y.detach().cpu())\n",
    "\n",
    "    # Combination tensor\n",
    "    result['features'] = torch.cat(result['features'], 0).numpy()\n",
    "    result['preds'] = torch.cat(result['preds'], 0).numpy()\n",
    "    result['ans'] = torch.cat(result['ans'], 0).numpy()\n",
    "\n",
    "    predstack = np.column_stack((result['features'], result['preds']))\n",
    "    targetstack = np.column_stack((result['features'], result['ans']))\n",
    "\n",
    "    return np.column_stack((predstack[:, -1], targetstack[:, -1]))\n",
    "\n",
    "def plot_error_rate(pred, target):\n",
    "    '''Plot error rate.'''\n",
    "    errors = (np.absolute(pred - target) / target) * 100\n",
    "    plt.plot(errors, '-r')\n",
    "\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Error Rate')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_pred(pred, target):\n",
    "    '''Plot predict.'''\n",
    "    plt.plot(pred, '-g', label='Prediction')\n",
    "    plt.plot(target, '--r', label='Target')\n",
    "\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Targets')\n",
    "    plt.title('Predictions')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_trend(pred, target):\n",
    "    X = np.array(pred).reshape(-1, 1)\n",
    "    y = np.array(target)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    plt.scatter(pred, target, c='b', label='Data')\n",
    "    plt.plot(pred, y_pred, c='r', label='Trend Line')\n",
    "\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Targets')\n",
    "    plt.title('Trend')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def tqdm_span(elapsed, total, n):\n",
    "    return elapsed * (total or 0) / max(n, 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock_Dataset(Dataset):\n",
    "    '''\n",
    "    x: Features.\n",
    "    y: Targets, if none, do prediction.\n",
    "    '''\n",
    "    def __init__(self, x, y=None, normalized=False, initscaler=True):\n",
    "        # Normalized\n",
    "        stack = np.column_stack((x, y))\n",
    "\n",
    "        if (normalized):\n",
    "            normalstack = scaler.fit_transform(stack) if initscaler else scaler.transform(stack)\n",
    "            x = normalstack[:, :-1]\n",
    "            y = y if y is None else normalstack[:, -1]\n",
    "\n",
    "        self.y = y if y is None else torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_dataloader(config, x, y=None, normalized=False, initscaler=True):\n",
    "    ''' Generates a dataset, then is put into a dataloader. '''\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y) if y is not None else y\n",
    "\n",
    "    dataset = Stock_Dataset(x, y, normalized, initscaler)\n",
    "    dataloader = DataLoader(dataset, **config['data_loader'])\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feat(train_data, valid_data, test_data, select_all=True):\n",
    "    '''Selects useful features to perform regression'''\n",
    "    y_train, y_valid, y_test = train_data[:,-1], valid_data[:,-1], test_data[:, -1]\n",
    "    raw_x_train, raw_x_valid, raw_x_test = train_data[:, :-1], valid_data[:, :-1], test_data[:, :-1]\n",
    "\n",
    "    if select_all:\n",
    "        feat_idx = list(range(raw_x_train.shape[1]))\n",
    "    else:\n",
    "        feat_idx = [0,1,2,3] # TODO: Select suitable feature columns.\n",
    "\n",
    "    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "train_data size: (3737, 6)\n",
      "valid_data size: (935, 6)\n",
      "test_data size: (1168, 6)\n",
      "number of features: 4\n"
     ]
    }
   ],
   "source": [
    "same_seed(config['seed'])\n",
    "\n",
    "# re-index\n",
    "index = [\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\", \"Close\"]\n",
    "raw_data = pd.DataFrame(yf.download('2330.TW', start='2000-01-01', end='2023-12-31'))\n",
    "# raw_data.to_csv(\"raw.csv\", index=False)\n",
    "train_data, test_data = train_test_split(\n",
    "    raw_data[index].values,\n",
    "    test_size=config['test_ratio'],\n",
    "    random_state=config['seed'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_data, valid_data = train_test_split(\n",
    "    train_data,\n",
    "    test_size=config['valid_ratio'],\n",
    "    random_state=config['seed'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Print out the data size.\n",
    "print(f\"\"\"train_data size: {train_data.shape}\n",
    "valid_data size: {valid_data.shape}\n",
    "test_data size: {test_data.shape}\"\"\")\n",
    "\n",
    "# Select features\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = select_feat(train_data, valid_data, test_data, config['select_all'])\n",
    "\n",
    "# Print out the number of features.\n",
    "print(f'number of features: {x_train.shape[1]}')\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = stock_dataloader(config, x_train, y_train)\n",
    "valid_loader = stock_dataloader(config, x_valid, y_valid, initscaler=False)\n",
    "test_loader = stock_dataloader(config, x_test, y_test, initscaler=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock_Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Stock_Model, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.7)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models')\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        postfix = [{\"Loss\": 0, \"Span\": \"\", \"Title\": \"Train Loss\"}, {\"Loss\": 0, \"Span\": \"\", \"Title\": \"Valid Loss\"}]\n",
    "        bar_format = \"{l_bar}{bar}| {n_fmt}/{total_fmt} {postfix[0][Span]} {postfix[0][Title]} {postfix[0][Loss]:>2.4f}, {postfix[1][Span]} {postfix[1][Title]} {postfix[1][Loss]:>2.4f}\"\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        loss_train_record = []\n",
    "\n",
    "        # with torch.inference_mode():\n",
    "        with tqdm(train_loader,\n",
    "                bar_format=bar_format,\n",
    "                desc=f'Epoch [{epoch+1}/{n_epochs}]',\n",
    "                postfix=postfix,\n",
    "                leave=False) as t:\n",
    "            for x, y in t:\n",
    "                optimizer.zero_grad()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "                loss_train_record.append(loss.detach().item())\n",
    "                mean_train_loss = sum(loss_train_record)/len(loss_train_record)\n",
    "\n",
    "                t.postfix[0][\"Loss\"] = mean_train_loss\n",
    "                t.postfix[0][\"Span\"] = f\"[{t.format_interval(tqdm_span(t.format_dict['elapsed'], t.format_dict['total'], t.format_dict['n']))}]\"\n",
    "                t.update()\n",
    "\n",
    "        writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "\n",
    "        # valid\n",
    "        model.eval()\n",
    "        loss_valid_record = []\n",
    "\n",
    "        with tqdm(valid_loader,\n",
    "                bar_format=bar_format,\n",
    "                desc=f'Epoch [{epoch+1}/{n_epochs}]',\n",
    "                postfix=postfix) as t:\n",
    "            for x, y in t:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.no_grad():\n",
    "                    pred = model(x)\n",
    "                    loss = criterion(pred, y)\n",
    "                    loss_valid_record.append(loss.item())\n",
    "                    mean_valid_loss = sum(loss_valid_record)/len(loss_valid_record)\n",
    "\n",
    "                    t.postfix[1][\"Loss\"] = mean_valid_loss\n",
    "                    t.postfix[1][\"Span\"] = f\"[{t.format_interval(tqdm_span(t.format_dict['elapsed'], t.format_dict['total'], t.format_dict['n']))}]\"\n",
    "                    t.update()\n",
    "\n",
    "        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5000]: 100%|██████████| 8/8 [00:00] Train Loss 10334.7887, [00:00] Valid Loss 9700.0710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 9700.071...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5000]: 100%|██████████| 8/8 [00:00] Train Loss 10199.9362, [00:00] Valid Loss 10215.7963\n",
      "Epoch [3/5000]: 100%|██████████| 8/8 [00:00] Train Loss 9957.7567, [00:00] Valid Loss 9593.1353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 9593.135...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5000]: 100%|██████████| 8/8 [00:00] Train Loss 9726.9792, [00:00] Valid Loss 9299.7559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 9299.756...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5000]: 100%|██████████| 8/8 [00:00] Train Loss 9247.8996, [00:00] Valid Loss 9029.3652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 9029.365...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/5000]: 100%|██████████| 8/8 [00:00] Train Loss 8759.7200, [00:00] Valid Loss 8363.1643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 8363.164...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/5000]: 100%|██████████| 8/8 [00:00] Train Loss 8274.3751, [00:00] Valid Loss 7518.2339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 7518.234...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/5000]: 100%|██████████| 8/8 [00:00] Train Loss 7616.4768, [00:00] Valid Loss 7036.3840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 7036.384...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/5000]: 100%|██████████| 8/8 [00:00] Train Loss 6517.1173, [00:00] Valid Loss 5611.0981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 5611.098...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000]: 100%|██████████| 8/8 [00:00] Train Loss 5486.7646, [00:00] Valid Loss 4884.0630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 4884.063...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/5000]: 100%|██████████| 8/8 [00:00] Train Loss 4529.3223, [00:00] Valid Loss 4057.2119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 4057.212...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/5000]: 100%|██████████| 8/8 [00:00] Train Loss 3645.8864, [00:00] Valid Loss 2789.0328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 2789.033...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/5000]: 100%|██████████| 8/8 [00:00] Train Loss 2706.3340, [00:00] Valid Loss 2240.4183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 2240.418...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/5000]: 100%|██████████| 8/8 [00:00] Train Loss 2036.3561, [00:00] Valid Loss 1541.2503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 1541.250...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/5000]: 100%|██████████| 8/8 [00:00] Train Loss 1497.8151, [00:00] Valid Loss 1631.4460\n",
      "Epoch [16/5000]: 100%|██████████| 8/8 [00:00] Train Loss 1035.7303, [00:00] Valid Loss 543.7464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 543.746...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/5000]: 100%|██████████| 8/8 [00:00] Train Loss 828.0910, [00:00] Valid Loss 270.9149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 270.915...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/5000]: 100%|██████████| 8/8 [00:00] Train Loss 570.4146, [00:00] Valid Loss 989.1570\n",
      "Epoch [19/5000]: 100%|██████████| 8/8 [00:00] Train Loss 395.6162, [00:00] Valid Loss 207.1596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 207.160...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/5000]: 100%|██████████| 8/8 [00:00] Train Loss 276.6129, [00:00] Valid Loss 557.7354\n",
      "Epoch [21/5000]: 100%|██████████| 8/8 [00:00] Train Loss 204.4479, [00:00] Valid Loss 274.7121\n",
      "Epoch [22/5000]: 100%|██████████| 8/8 [00:00] Train Loss 173.9715, [00:00] Valid Loss 93.1218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 93.122...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [23/5000]: 100%|██████████| 8/8 [00:00] Train Loss 122.8432, [00:00] Valid Loss 68.0684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 68.068...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [24/5000]: 100%|██████████| 8/8 [00:00] Train Loss 99.8710, [00:00] Valid Loss 28.1981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 28.198...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [25/5000]: 100%|██████████| 8/8 [00:00] Train Loss 85.0881, [00:00] Valid Loss 49.1583\n",
      "Epoch [26/5000]: 100%|██████████| 8/8 [00:00] Train Loss 64.3533, [00:00] Valid Loss 20.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with loss 20.745...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m Stock_Model(input_dim\u001b[39m=\u001b[39mx_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m trainer(train_loader, valid_loader, model, config, device)\n\u001b[1;32m      4\u001b[0m \u001b[39m# with torch.profiler.profile(\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#     activities=[torch.profiler.ProfilerActivity.CPU],\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#     schedule = torch.profiler.schedule(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m#     trainer(train_loader, valid_loader, model, config, device)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m#     prof.step()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 31\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(train_loader, valid_loader, model, config, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     30\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     32\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, y)\n\u001b[1;32m     33\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mStock_Model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n\u001b[1;32m     16\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:151\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats:\n\u001b[1;32m    149\u001b[0m     \u001b[39m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_batches_tracked\u001b[39m.\u001b[39;49madd_(\u001b[39m1\u001b[39;49m)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    152\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# use cumulative moving average\u001b[39;00m\n\u001b[1;32m    153\u001b[0m             exponential_average_factor \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Stock_Model(input_dim=x_train.shape[1]).to(device)\n",
    "trainer(train_loader, valid_loader, model, config, device)\n",
    "\n",
    "# with torch.profiler.profile(\n",
    "#     activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "#     schedule = torch.profiler.schedule(\n",
    "#         wait=0,\n",
    "#         warmup=0,\n",
    "#         active=1\n",
    "#     ),\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "# ) as prof:\n",
    "#     trainer(train_loader, valid_loader, model, config, device)\n",
    "#     prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./log/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curves with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    np.savetxt(file, preds, delimiter=',', fmt='%d', header='pred,target', comments='')\n",
    "\n",
    "model = Stock_Model(input_dim=x_train.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader, model, device)\n",
    "save_pred(preds, 'pred.csv')\n",
    "plot_error_rate(preds[:,0], preds[:,1])\n",
    "plot_pred(preds[:,0], preds[:,1])\n",
    "plot_trend(preds[:,0], preds[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Stock_Model(input_dim=x_train.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "\n",
    "single_loader = stock_dataloader(config, [[100,110,90,101]], [103])\n",
    "predict(single_loader, model, device)\n",
    "\n",
    "# for pred, target in single_pred:\n",
    "#     print(f\"Prediction: {pred}, Target: {target}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
